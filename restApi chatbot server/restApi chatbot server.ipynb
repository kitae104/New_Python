{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "from tensorflow.keras import preprocessing\n",
    "import pathlib\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path \n",
    "import re\n",
    "from datetime import datetime\n",
    "from konlpy.tag import Komoran\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !필독!\n",
    "\n",
    "1. 학습을 시켰을때와 대체로 비슷한 구조의 파일입니다. 다만 학습시키는 과정과, 기타 전처리 과정들이 빠져있을뿐..\n",
    "2. 주의할 점은 학습시켰을때 썼던 파일 순서대로 dataset 리스트에 넣어주세요. 안그러면 convert_to_originalSentense() 썼을때 key error오류날껍니다. 또한 학습 시킨 갯수(trainCount)도 동일하게 해주셔야 합니다\n",
    " - ex) ['1.xlxs', '2.xlsx', '3.xlsx'] -> 학습 시켰을 때 / ['1.xlxs', '2.xlsx', '3.xlsx'] -> 현재 파일에서 예측 시킬때\n",
    "3. RestAPI는 플라스크에서 제공하는 CORS를 사용했습니다. 안그러면 크롬이나 기타 브라우저에서 오류납니다. pip로 깔아주세요\n",
    "4. 만약 하다가 데이터셋이 꼬였다 싶으면... datatSetClean() 함수를 통해서 데이터셋 클리어 해주시고 setDataSet() 함수로 다시 설정해주면 될껍니다\n",
    "5. 주소는 바꾸되 Post로 안하면 사용자의 돌발행동으로 인해서 오류가 날수도 있습니다. (질문이 1000자정도 넘어갈때)\n",
    "6. 플라스크는 테스트 안해봤습니다!! 입맛에 따라 바꾸세요!! 아마 안될리가 없긴 합니다만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatbot:\n",
    "    PAD = \"<PADDING>\"   # 패딩\n",
    "    STA = \"<START>\"     # 시작\n",
    "    END = \"<END>\"       # 끝\n",
    "    OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
    "    # 태그 인덱스\n",
    "    PAD_INDEX = 0\n",
    "    STA_INDEX = 1\n",
    "    END_INDEX = 2\n",
    "    OOV_INDEX = 3\n",
    "    # 데이터 타입\n",
    "    ENCODER_INPUT  = 0\n",
    "    DECODER_INPUT  = 1\n",
    "    DECODER_TARGET = 2\n",
    "    # 한 문장에서 단어 시퀀스의 최대 개수\n",
    "    max_sequences = 100\n",
    "    RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n",
    "    dataSet = pd.DataFrame([])\n",
    "    question = []\n",
    "    answer = []\n",
    "    word_to_index = None\n",
    "    index_to_word = None\n",
    "    x_encoder = None\n",
    "    x_decoder = None\n",
    "    y_decoder = None\n",
    "    model = None\n",
    "    encoder_model = None\n",
    "    decoder_model = None\n",
    "    originalDict = {}\n",
    "    \n",
    "    def setDataSet(self, excelFile):\n",
    "        # 데이터 셋 집어넣을 때 반드시 컬럼명을 Q, A로 지정해주셔야 합니다.\n",
    "        # 또한 엑셀파일 전용입니다.\n",
    "        self.dataSet = self.dataSet.append(pd.read_excel(excelFile,encoding='utf-8'))\n",
    "        self.question, self.answer = list(self.dataSet['Q']), list(self.dataSet['A'])\n",
    "    def dataSetClean(self):\n",
    "        self.dataSet = pd.DataFrame([])\n",
    "    def setMaxSequences(self,maxSequences):\n",
    "        self.max_sequences = maxSequences\n",
    "    def setEmbedding_dim(self, embeddingDim):\n",
    "        self.embedding_dim = embeddingDim\n",
    "    # 형태소분석 함수\n",
    "    def pos_tag(self, sentences):\n",
    "        # KoNLPy 형태소분석기 설정\n",
    "        tagger = Komoran()\n",
    "        # 문장 품사 변수 초기화\n",
    "        sentences_pos = []\n",
    "        # 모든 문장 반복\n",
    "        for sentence in sentences:\n",
    "            # 특수기호 제거\n",
    "            sentence = re.sub(self.RE_FILTER, \"\", sentence)\n",
    "            # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "            sentence = \" \".join(tagger.morphs(sentence))\n",
    "            sentences_pos.append(sentence)\n",
    "        return sentences_pos\n",
    "    def startPosTag(self):\n",
    "        self.question = self.pos_tag(self.question)\n",
    "        self.answer = self.pos_tag(self.answer)\n",
    "       \n",
    "    def getSetUpfile(self, word_to_index, index_to_word, x_encoder, x_decoder, y_decoder, model, encoder_model, decoder_model):\n",
    "        file = pathlib.Path(word_to_index)\n",
    "        file_text = file.read_text(encoding='utf-8')\n",
    "        self.word_to_index = json.loads(file_text)\n",
    "        \n",
    "        file = pathlib.Path(index_to_word)\n",
    "        file_text = file.read_text(encoding='utf-8')\n",
    "        self.index_to_word = json.loads(file_text)\n",
    "        self.index_to_word = {index:word for index,word in enumerate(self.index_to_word.values())}\n",
    "        \n",
    "        self.x_encoder = pd.read_csv(x_encoder)\n",
    "        del(self.x_encoder['Unnamed: 0'])\n",
    "        self.x_encoder = np.array(self.x_encoder)\n",
    "        \n",
    "        self.x_decoder = pd.read_csv(x_decoder)\n",
    "        del(self.x_decoder['Unnamed: 0'])\n",
    "        self.x_decoder = np.array(self.x_decoder)\n",
    "        \n",
    "        self.y_decoder = pd.read_csv(y_decoder)\n",
    "        del(self.y_decoder['Unnamed: 0'])\n",
    "        self.y_decoder = np.array(self.y_decoder)\n",
    "        \n",
    "        self.model = load_model(model) \n",
    "        self.encoder_model = load_model(encoder_model)\n",
    "        self.decoder_model = load_model(decoder_model)\n",
    "        \n",
    "    def convert_text_to_index(self, sentences, vocabulary, type): \n",
    "        sentences_index = []\n",
    "        # 모든 문장에 대해서 반복\n",
    "        for sentence in sentences:\n",
    "            sentence_index = []\n",
    "            # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
    "            if type == self.DECODER_INPUT:\n",
    "                sentence_index.extend([vocabulary[self.STA]])\n",
    "\n",
    "            # 문장의 단어들을 띄어쓰기로 분리\n",
    "            for word in sentence.split():\n",
    "                if vocabulary.get(word) is not None:\n",
    "                    # 사전에 있는 단어면 해당 인덱스를 추가\n",
    "                    sentence_index.extend([vocabulary[word]])\n",
    "                else:\n",
    "                    # 사전에 없는 단어면 OOV 인덱스를 추가\n",
    "                    sentence_index.extend([vocabulary[self.OOV]])\n",
    "\n",
    "            # 최대 길이 검사\n",
    "            if type == self.DECODER_TARGET:\n",
    "                # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
    "                if len(sentence_index) >= self.max_sequences:\n",
    "                    sentence_index = sentence_index[:self.max_sequences-1] + [vocabulary[self.END]]\n",
    "                else:\n",
    "                    sentence_index += [vocabulary[self.END]]\n",
    "            else:\n",
    "                if len(sentence_index) > self.max_sequences:\n",
    "                    sentence_index = sentence_index[:self.max_sequences]\n",
    "\n",
    "            # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
    "            sentence_index += (self.max_sequences - len(sentence_index)) * [vocabulary[self.PAD]]\n",
    "\n",
    "            # 문장의 인덱스 배열을 추가\n",
    "            sentences_index.append(sentence_index)\n",
    "\n",
    "        return np.asarray(sentences_index)\n",
    "   \n",
    "    \n",
    "    def convert_index_to_text(self, indexs, vocabulary): \n",
    "    \n",
    "        sentence = ''\n",
    "\n",
    "        # 모든 문장에 대해서 반복\n",
    "        for index in indexs:\n",
    "            if index == self.END_INDEX:\n",
    "                # 종료 인덱스면 중지\n",
    "                break;\n",
    "            if vocabulary.get(index) is not None:\n",
    "                # 사전에 있는 인덱스면 해당 단어를 추가\n",
    "                sentence += vocabulary[index]\n",
    "            else:\n",
    "                # 사전에 없는 인덱스면 OOV 단어를 추가\n",
    "                sentence.extend([vocabulary[self.OOV_INDEX]])\n",
    "\n",
    "            # 빈칸 추가\n",
    "            sentence += ' '\n",
    "\n",
    "        return sentence\n",
    "    def make_predict_input(self, sentence):\n",
    "\n",
    "        sentences = []\n",
    "        sentences.append(sentence)\n",
    "        sentences = self.pos_tag(sentences)\n",
    "        input_seq = self.convert_text_to_index(sentences, self.word_to_index, self.ENCODER_INPUT)\n",
    "\n",
    "        return input_seq\n",
    "    # 텍스트 생성\n",
    "    def generate_text(self, input_seq):\n",
    "\n",
    "        # 입력을 인코더에 넣어 마지막 상태 구함\n",
    "        states = self.encoder_model.predict(input_seq)\n",
    "\n",
    "        # 목표 시퀀스 초기화\n",
    "        target_seq = np.zeros((1, 1))\n",
    "\n",
    "        # 목표 시퀀스의 첫 번째에 <START> 태그 추가\n",
    "        target_seq[0, 0] = self.STA_INDEX\n",
    "\n",
    "        # 인덱스 초기화\n",
    "        indexs = []\n",
    "\n",
    "        # 디코더 타임 스텝 반복\n",
    "        while 1:\n",
    "            # 디코더로 현재 타임 스텝 출력 구함\n",
    "            # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
    "            decoder_outputs, state_h, state_c = self.decoder_model.predict(\n",
    "                                                    [target_seq] + states)\n",
    "\n",
    "            # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "            index = np.argmax(decoder_outputs[0, 0, :])\n",
    "            indexs.append(index)\n",
    "\n",
    "            # 종료 검사\n",
    "            if index == self.END_INDEX or len(indexs) >= self.max_sequences:\n",
    "                break\n",
    "\n",
    "            # 목표 시퀀스를 바로 이전의 출력으로 설정\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = index\n",
    "\n",
    "            # 디코더의 이전 상태를 다음 디코더 예측에 사용\n",
    "            states = [state_h, state_c]\n",
    "\n",
    "        # 인덱스를 문장으로 변환\n",
    "        sentence = self.convert_index_to_text(indexs, self.index_to_word)\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "    def makePredictSentense(self,targetSentense):\n",
    "        return self.generate_text(self.make_predict_input(targetSentense))\n",
    "    \n",
    "    def makeOriginalSentenseDict(self, targetSentenseList):\n",
    "        for sentense in targetSentenseList :\n",
    "            self.originalDict[self.makePredictSentense(sentense)] = sentense\n",
    "            \n",
    "    def convert_to_originalSentense(self, predictSentense):\n",
    "        sentense = self.makePredictSentense(predictSentense)\n",
    "        return self.originalDict[sentense]\n",
    "    \n",
    "    #\n",
    "    def AutoStart(self, dataSet,trainCount, word_to_index, index_to_word, x_encoder, x_decoder, y_decoder, model, encoder_model, decoder_model):\n",
    "        for i in dataSet:\n",
    "            self.setDataSet(i)\n",
    "        self.startPosTag()\n",
    "        self.getSetUpfile(word_to_index, index_to_word, x_encoder, x_decoder, y_decoder, model, encoder_model, decoder_model)\n",
    "        self.makeOriginalSentenseDict(self.dataSet['A'][:trainCount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['dataset/chatbot/예제.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/chatbot/예제.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f128f1623a93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mx_decoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./EnDecoder/20200815_x_decoder.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_decoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./EnDecoder/20200815_y_decoder.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./model/20200815_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./model/20200815_encoder_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             decoder_model = './model/20200815_decoder_model.h5')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-34ab22eb9caf>\u001b[0m in \u001b[0;36mAutoStart\u001b[1;34m(self, dataSet, trainCount, word_to_index, index_to_word, x_encoder, x_decoder, y_decoder, model, encoder_model, decoder_model)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mAutoStart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainCount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartPosTag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSetUpfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-34ab22eb9caf>\u001b[0m in \u001b[0;36msetDataSet\u001b[1;34m(self, excelFile)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# 데이터 셋 집어넣을 때 반드시 컬럼명을 Q, A로 지정해주셔야 합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# 또한 엑셀파일 전용입니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexcelFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Q'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdataSetClean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\37\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mc:\\python\\37\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, engine)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\37\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlrd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\37\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\37\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\37\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/chatbot/예제.xlsx'"
     ]
    }
   ],
   "source": [
    "# 요건 예제이니 바꾸시면 됩니다.\n",
    "cb.AutoStart(dataset, trainCount=10, index_to_word = './json/20200815_index_to_word.json',\n",
    "            word_to_index = './json/20200815_word_to_index.json', x_encoder = './EnDecoder/20200815_x_encoder.csv',\n",
    "            x_decoder = './EnDecoder/20200815_x_decoder.csv', y_decoder = './EnDecoder/20200815_y_decoder.csv', \n",
    "            model = './model/20200815_model.h5', encoder_model = './model/20200815_encoder_model.h5',\n",
    "            decoder_model = './model/20200815_decoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'코로나바이러스 감염증 - 19 코로나 19 는 과거 에 발견 되 지 않 았 던 새롭 ㄴ 코로나바이러스 이 ㄴ SARS - CoV -2 에 의하 아 발생 하 는 호흡기 감염병 입 니다 이 바이러스 에 감염 되 면 무증상 부터 중증 에 이르 기 까지 다양 하 ㄴ 임상 증상 이 나타나 ㄹ 수 있 습니다 이 새롭 ㄴ 바이러스 와 질병 은 2019 년 12월 중국 우 한 에서 처음 보고 되 었 고 현재 전 세계 에 확산 되 었 습니다 '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.makePredictSentense('코로나란 무엇인가요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "from flask_cors import CORS\n",
    "from datetime import datetime\n",
    "@app.route('/dto/',methods = ['POST'])\n",
    "def chatbotServer():\n",
    "    Q = request.form['Q']\n",
    "    print(Q)\n",
    "    A = cb.convert_to_originalSentense(Q)\n",
    "    return jsonify({'Q' : Q, 'A' : A, 'datetime' : datetime.today().strftime(\"%Y%m%d%H%M%S\")})\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    CORS(app)\n",
    "    app.run(port=5000 , threaded = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
