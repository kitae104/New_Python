{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "from tensorflow.keras import preprocessing\n",
    "import pathlib\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path \n",
    "import re\n",
    "from datetime import datetime\n",
    "from konlpy.tag import Komoran\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !필독!\n",
    "\n",
    "1. 학습을 시켰을때와 대체로 비슷한 구조의 파일입니다. 다만 학습시키는 과정과, 기타 전처리 과정들이 빠져있을뿐..\n",
    "2. 주의할 점은 학습시켰을때 썼던 파일 순서대로 dataset 리스트에 넣어주세요. 안그러면 convert_to_originalSentense() 썼을때 key error오류날껍니다. 또한 학습 시킨 갯수(trainCount)도 동일하게 해주셔야 합니다\n",
    " - ex) ['1.xlxs', '2.xlsx', '3.xlsx'] -> 학습 시켰을 때 / ['1.xlxs', '2.xlsx', '3.xlsx'] -> 현재 파일에서 예측 시킬때\n",
    "3. RestAPI는 플라스크에서 제공하는 CORS를 사용했습니다. 안그러면 크롬이나 기타 브라우저에서 오류납니다. pip로 깔아주세요\n",
    "4. 만약 하다가 데이터셋이 꼬였다 싶으면... datatSetClean() 함수를 통해서 데이터셋 클리어 해주시고 setDataSet() 함수로 다시 설정해주면 될껍니다\n",
    "5. 주소는 바꾸되 Post로 안하면 사용자의 돌발행동으로 인해서 오류가 날수도 있습니다. (질문이 1000자정도 넘어갈때)\n",
    "6. 플라스크는 테스트 안해봤습니다!! 입맛에 따라 바꾸세요!! 아마 안될리가 없긴 합니다만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatbot:\n",
    "    PAD = \"<PADDING>\"   # 패딩\n",
    "    STA = \"<START>\"     # 시작\n",
    "    END = \"<END>\"       # 끝\n",
    "    OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
    "    # 태그 인덱스\n",
    "    PAD_INDEX = 0\n",
    "    STA_INDEX = 1\n",
    "    END_INDEX = 2\n",
    "    OOV_INDEX = 3\n",
    "    # 데이터 타입\n",
    "    ENCODER_INPUT  = 0\n",
    "    DECODER_INPUT  = 1\n",
    "    DECODER_TARGET = 2\n",
    "    # 한 문장에서 단어 시퀀스의 최대 개수\n",
    "    max_sequences = 100\n",
    "    RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n",
    "    dataSet = pd.DataFrame([])\n",
    "    question = []\n",
    "    answer = []\n",
    "    word_to_index = None\n",
    "    index_to_word = None\n",
    "    x_encoder = None\n",
    "    x_decoder = None\n",
    "    y_decoder = None\n",
    "    model = None\n",
    "    encoder_model = None\n",
    "    decoder_model = None\n",
    "    originalDict = {}\n",
    "    \n",
    "    def setDataSet(self, excelFile):\n",
    "        # 데이터 셋 집어넣을 때 반드시 컬럼명을 Q, A로 지정해주셔야 합니다.\n",
    "        # 또한 엑셀파일 전용입니다.\n",
    "        self.dataSet = self.dataSet.append(pd.read_excel(excelFile,encoding='utf-8'))\n",
    "        self.question, self.answer = list(self.dataSet['Q']), list(self.dataSet['A'])\n",
    "    def dataSetClean(self):\n",
    "        self.dataSet = pd.DataFrame([])\n",
    "    def setMaxSequences(self,maxSequences):\n",
    "        self.max_sequences = maxSequences\n",
    "    def setEmbedding_dim(self, embeddingDim):\n",
    "        self.embedding_dim = embeddingDim\n",
    "    # 형태소분석 함수\n",
    "    def pos_tag(self, sentences):\n",
    "        # KoNLPy 형태소분석기 설정\n",
    "        tagger = Komoran()\n",
    "        # 문장 품사 변수 초기화\n",
    "        sentences_pos = []\n",
    "        # 모든 문장 반복\n",
    "        for sentence in sentences:\n",
    "            # 특수기호 제거\n",
    "            sentence = re.sub(self.RE_FILTER, \"\", sentence)\n",
    "            # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "            sentence = \" \".join(tagger.morphs(sentence))\n",
    "            sentences_pos.append(sentence)\n",
    "        return sentences_pos\n",
    "    def startPosTag(self):\n",
    "        self.question = self.pos_tag(self.question)\n",
    "        self.answer = self.pos_tag(self.answer)\n",
    "       \n",
    "    def getSetUpfile(self, word_to_index, index_to_word, x_encoder, x_decoder, y_decoder, model, encoder_model, decoder_model):\n",
    "        file = pathlib.Path(word_to_index)\n",
    "        file_text = file.read_text(encoding='utf-8')\n",
    "        self.word_to_index = json.loads(file_text)\n",
    "        \n",
    "        file = pathlib.Path(index_to_word)\n",
    "        file_text = file.read_text(encoding='utf-8')\n",
    "        self.index_to_word = json.loads(file_text)\n",
    "        self.index_to_word = {index:word for index,word in enumerate(self.index_to_word.values())}\n",
    "        \n",
    "        self.x_encoder = pd.read_csv(x_encoder)\n",
    "        del(self.x_encoder['Unnamed: 0'])\n",
    "        self.x_encoder = np.array(self.x_encoder)\n",
    "        \n",
    "        self.x_decoder = pd.read_csv(x_decoder)\n",
    "        del(self.x_decoder['Unnamed: 0'])\n",
    "        self.x_decoder = np.array(self.x_decoder)\n",
    "        \n",
    "        self.y_decoder = pd.read_csv(y_decoder)\n",
    "        del(self.y_decoder['Unnamed: 0'])\n",
    "        self.y_decoder = np.array(self.y_decoder)\n",
    "        \n",
    "        self.model = load_model(model) \n",
    "        self.encoder_model = load_model(encoder_model)\n",
    "        self.decoder_model = load_model(decoder_model)\n",
    "        \n",
    "    def convert_text_to_index(self, sentences, vocabulary, type): \n",
    "        sentences_index = []\n",
    "        # 모든 문장에 대해서 반복\n",
    "        for sentence in sentences:\n",
    "            sentence_index = []\n",
    "            # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
    "            if type == self.DECODER_INPUT:\n",
    "                sentence_index.extend([vocabulary[self.STA]])\n",
    "\n",
    "            # 문장의 단어들을 띄어쓰기로 분리\n",
    "            for word in sentence.split():\n",
    "                if vocabulary.get(word) is not None:\n",
    "                    # 사전에 있는 단어면 해당 인덱스를 추가\n",
    "                    sentence_index.extend([vocabulary[word]])\n",
    "                else:\n",
    "                    # 사전에 없는 단어면 OOV 인덱스를 추가\n",
    "                    sentence_index.extend([vocabulary[self.OOV]])\n",
    "\n",
    "            # 최대 길이 검사\n",
    "            if type == self.DECODER_TARGET:\n",
    "                # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
    "                if len(sentence_index) >= self.max_sequences:\n",
    "                    sentence_index = sentence_index[:self.max_sequences-1] + [vocabulary[self.END]]\n",
    "                else:\n",
    "                    sentence_index += [vocabulary[self.END]]\n",
    "            else:\n",
    "                if len(sentence_index) > self.max_sequences:\n",
    "                    sentence_index = sentence_index[:self.max_sequences]\n",
    "\n",
    "            # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
    "            sentence_index += (self.max_sequences - len(sentence_index)) * [vocabulary[self.PAD]]\n",
    "\n",
    "            # 문장의 인덱스 배열을 추가\n",
    "            sentences_index.append(sentence_index)\n",
    "\n",
    "        return np.asarray(sentences_index)\n",
    "   \n",
    "    \n",
    "    def convert_index_to_text(self, indexs, vocabulary): \n",
    "    \n",
    "        sentence = ''\n",
    "\n",
    "        # 모든 문장에 대해서 반복\n",
    "        for index in indexs:\n",
    "            if index == self.END_INDEX:\n",
    "                # 종료 인덱스면 중지\n",
    "                break;\n",
    "            if vocabulary.get(index) is not None:\n",
    "                # 사전에 있는 인덱스면 해당 단어를 추가\n",
    "                sentence += vocabulary[index]\n",
    "            else:\n",
    "                # 사전에 없는 인덱스면 OOV 단어를 추가\n",
    "                sentence.extend([vocabulary[self.OOV_INDEX]])\n",
    "\n",
    "            # 빈칸 추가\n",
    "            sentence += ' '\n",
    "\n",
    "        return sentence\n",
    "    def make_predict_input(self, sentence):\n",
    "\n",
    "        sentences = []\n",
    "        sentences.append(sentence)\n",
    "        sentences = self.pos_tag(sentences)\n",
    "        input_seq = self.convert_text_to_index(sentences, self.word_to_index, self.ENCODER_INPUT)\n",
    "\n",
    "        return input_seq\n",
    "    # 텍스트 생성\n",
    "    def generate_text(self, input_seq):\n",
    "\n",
    "        # 입력을 인코더에 넣어 마지막 상태 구함\n",
    "        states = self.encoder_model.predict(input_seq)\n",
    "\n",
    "        # 목표 시퀀스 초기화\n",
    "        target_seq = np.zeros((1, 1))\n",
    "\n",
    "        # 목표 시퀀스의 첫 번째에 <START> 태그 추가\n",
    "        target_seq[0, 0] = self.STA_INDEX\n",
    "\n",
    "        # 인덱스 초기화\n",
    "        indexs = []\n",
    "\n",
    "        # 디코더 타임 스텝 반복\n",
    "        while 1:\n",
    "            # 디코더로 현재 타임 스텝 출력 구함\n",
    "            # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
    "            decoder_outputs, state_h, state_c = self.decoder_model.predict(\n",
    "                                                    [target_seq] + states)\n",
    "\n",
    "            # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "            index = np.argmax(decoder_outputs[0, 0, :])\n",
    "            indexs.append(index)\n",
    "\n",
    "            # 종료 검사\n",
    "            if index == self.END_INDEX or len(indexs) >= self.max_sequences:\n",
    "                break\n",
    "\n",
    "            # 목표 시퀀스를 바로 이전의 출력으로 설정\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = index\n",
    "\n",
    "            # 디코더의 이전 상태를 다음 디코더 예측에 사용\n",
    "            states = [state_h, state_c]\n",
    "\n",
    "        # 인덱스를 문장으로 변환\n",
    "        sentence = self.convert_index_to_text(indexs, self.index_to_word)\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "    def makePredictSentense(self,targetSentense):\n",
    "        return self.generate_text(self.make_predict_input(targetSentense))\n",
    "    \n",
    "    def makeOriginalSentenseDict(self, targetSentenseList):\n",
    "        for sentense in targetSentenseList :\n",
    "            self.originalDict[self.makePredictSentense(sentense)] = sentense\n",
    "            \n",
    "    def convert_to_originalSentense(self, predictSentense):\n",
    "        sentense = self.makePredictSentense(predictSentense)\n",
    "        return self.originalDict[sentense]\n",
    "    \n",
    "    #\n",
    "    def AutoStart(self, dataSet,trainCount, word_to_index, index_to_word, x_encoder, x_decoder, y_decoder, model, encoder_model, decoder_model):\n",
    "        for i in dataSet:\n",
    "            self.setDataSet(i)\n",
    "        self.startPosTag()\n",
    "        self.getSetUpfile(word_to_index, index_to_word, x_encoder, x_decoder, y_decoder, model, encoder_model, decoder_model)\n",
    "        self.makeOriginalSentenseDict(self.dataSet['A'][:trainCount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['dataset/chatbot/예제.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# 요건 예제이니 바꾸시면 됩니다.\n",
    "cb.AutoStart(dataset, trainCount=10, index_to_word = './json/20200815_index_to_word.json',\n",
    "            word_to_index = './json/20200815_word_to_index.json', x_encoder = './EnDecoder/20200815_x_encoder.csv',\n",
    "            x_decoder = './EnDecoder/20200815_x_decoder.csv', y_decoder = './EnDecoder/20200815_y_decoder.csv', \n",
    "            model = './model/20200815_model.h5', encoder_model = './model/20200815_encoder_model.h5',\n",
    "            decoder_model = './model/20200815_decoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'체육 체육 포함 체육 전 운동 운동 나가 보통 기 속 속 를 로 아서 사회 사회 를 를 대 1 링 함께 보통 온라인 에게 강도 강도 다면 새롭 윗몸 윗몸 윗몸 근력 윗몸 근력 전 휴식 올바르 올바르 크 기술 에게 에게 당뇨병 체육 추천 시작 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 이하 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.makePredictSentense('코로나란 무엇인가요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask_cors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5dced8676a0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjsonify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mflask_cors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCORS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/dto/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'POST'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flask_cors'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "from flask_cors import CORS\n",
    "from datetime import datetime\n",
    "@app.route('/dto/',methods = ['POST'])\n",
    "def chatbotServer():\n",
    "    Q = request.form['Q']\n",
    "    print(Q)\n",
    "    A = cb.convert_to_originalSentense(Q)\n",
    "    return jsonify({'Q' : Q, 'A' : A, 'datetime' : datetime.today().strftime(\"%Y%m%d%H%M%S\")})\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    CORS(app)\n",
    "    app.run(port=5000 , threaded = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
