{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from konlpy.tag import Komoran\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### !필독!\n",
    "\n",
    "1. py 형태로 바꿔서 import 시키든 그냥 여기서 하던 될껍니다\n",
    "\n",
    "2. 데이터 셋 형식은 반드시 xlsx 파일로 하시는게 좋습니다. csv나 기타 확장자면 직접 코드 수정하세요\n",
    "\n",
    "3. 학습 반복 횟수는 적어도 1500번정도는 설정해야됩니다. 안그러면 이상한 대답이 튀어나옵니다\n",
    "\n",
    "4. 대략 800개 문장을 학습시켰을때 하루 걸렸습니다.(gtx1070, GPU 가속모드 사용)정확도는 0.999, 손실률은 0.001 이하로 나오면 꽤 정확한 문장을 얻을 수 있습니다\n",
    "\n",
    "5. 팁을 드리자면 질문의 조사등을 바꾸던가 문장 순서를 바꾸던가를 해서 비슷한 질문 : 대답(이건 본래 대답과 똑같이)을 여러개 만드시면 꽤 광범위한 질문들을 커버할 수 있습니다\n",
    "\n",
    "6. 만약에 가장 긴 질문안에 들어가는 단어의 갯수가 100개 이상이면 max_sequences, 변수를 단어 최대 갯수에 맞게 수정해주세요\n",
    "\n",
    "7. 정말 귀찮으신 분들을 위해서 AutoStart를 만들어놨습니다. 이거 함수 하나면 학습부터 예측 딕셔너리 설정까지 한큐에 됩니다. 대신 쓸 엑셀파일들은 리스트 형식으로 해주세요 (ex. ['./데이터셋.xlxs', './데이터셋2.xlsx'])\n",
    "\n",
    "8. 한가지 팁을 더 드리자면 문장안에 들어가는 단어의 갯수가 적으면 적을수록 학습시간이 줄어듭니다. \n",
    "\n",
    "9. 파일 구조 - 구조를 지켜주세요.\n",
    " - ./dataset/chatbot : 데이터셋을 모으는 저장소\n",
    " - EnDecoder : 인코더 파일과 디코더 파일들 저장소\n",
    " - json : 인덱스 : 단어 / 단어 : 인덱스 형태의 .json 파일이 저장되는 저장소\n",
    " - model : 학습, 예측 모델이 저장되는 저장소\n",
    " \n",
    " \n",
    "#### 마지막으로 아래 쓴 함수 순서대로 (위에서 부터 아래) 진행해 주세요 안그럼 설정 오류 날껍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 세트 설정 (형식은 xlxs 파일)\n",
    "setDataSet()\n",
    "#### 데이터 세트 konlpy Kommoran으로 품사별 태깅\n",
    "startPosTag()\n",
    "#### 대답과 질문 하나로 합치는 과정\n",
    "setTrainSentences()\n",
    "#### 인덱스 :  단어 , 인덱스 : 단어 형태의 딕셔너리로 만드는 과정 (산출물 : 해당날짜의 json 파일들)\n",
    "makeDict()\n",
    "#### 데이터세트를 encoder와 decoder로 변환.(산출물 : 해당 날짜의 csv 파일들)\n",
    "makeEnDeCoder()\n",
    "#### 예측 디코더를 원핫인코딩으로 변환\n",
    "makeYdecoderToOneHot()\n",
    "#### 학습 모델 설정 (산출물 : 학습 모델의 .h5파일)\n",
    "trainModelInit()\n",
    "#### 예측 모델 설정 (산출물 : 예측 encoder, decoder 의 .h5 파일)\n",
    "predictModelInit()\n",
    "#### 학습 시작\n",
    "startTrain(반복 학습 갯수,학습시킬 데이터 갯수)\n",
    "#### 예측 문장을 원복시켜주는 함수( 이 함수 사용 이유는 학습시킬때 썼던 문장이 품사별로 태깅된 문장이기에 원래문장으로 원복시켜주는 함수이다)\n",
    "makeOriginalSentenseDict(학습시켰던 데이터 갯수)\n",
    "\n",
    "#### 질문을 넣으면 원복된 대답이 나오는 함수\n",
    "convert_to_originalSentense(질문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatbot:\n",
    "    PAD = \"<PADDING>\"   # 패딩\n",
    "    STA = \"<START>\"     # 시작\n",
    "    END = \"<END>\"       # 끝\n",
    "    OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
    "    # 태그 인덱스\n",
    "    PAD_INDEX = 0\n",
    "    STA_INDEX = 1\n",
    "    END_INDEX = 2\n",
    "    OOV_INDEX = 3\n",
    "    # 데이터 타입\n",
    "    ENCODER_INPUT  = 0\n",
    "    DECODER_INPUT  = 1\n",
    "    DECODER_TARGET = 2\n",
    "    # 한 문장에서 단어 시퀀스의 최대 개수\n",
    "    max_sequences = 100\n",
    "    # 임베딩 벡터 차원\n",
    "    embedding_dim = 100\n",
    "        # LSTM 히든레이어 차원\n",
    "    lstm_hidden_dim = 128\n",
    "        # 정규 표현식 필터\n",
    "    RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n",
    "    dataSet = pd.DataFrame([])\n",
    "    question = []\n",
    "    answer = []\n",
    "    sentences = []\n",
    "    words = []\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    x_encoder = np.array([])\n",
    "    x_decoder = np.array([])\n",
    "    y_decoder = np.array([])\n",
    "    v_loss= []\n",
    "    v_acc= []\n",
    "    acc= []\n",
    "    loss = []\n",
    "    model = None\n",
    "    encoder_inputs = None\n",
    "    encoder_states = None\n",
    "    decoder_inputs = None\n",
    "    decoder_embedding = None\n",
    "    decoder_lstm = None\n",
    "    decoder_dense = None\n",
    "    encoder_model = None\n",
    "    decoder_model = None\n",
    "    originalDict = {}\n",
    "    \n",
    "    def setDataSet(self, excelFile):\n",
    "        # 데이터 셋 집어넣을 때 반드시 컬럼명을 Q, A로 지정해주셔야 합니다.\n",
    "        # 또한 엑셀파일 전용입니다.\n",
    "        self.dataSet = self.dataSet.append(pd.read_excel(excelFile,encoding='utf-8'))\n",
    "        self.question, self.answer = list(self.dataSet['Q']), list(self.dataSet['A'])\n",
    "    def dataSetClean(self):\n",
    "        self.dataSet = pd.DataFrame([])\n",
    "    def setMaxSequences(self,maxSequences):\n",
    "        self.max_sequences = maxSequences\n",
    "    def setEmbedding_dim(self, embeddingDim):\n",
    "        self.embedding_dim = embeddingDim\n",
    "    # 형태소분석 함수\n",
    "    def pos_tag(self, sentences):\n",
    "        # KoNLPy 형태소분석기 설정\n",
    "        tagger = Komoran()\n",
    "        # 문장 품사 변수 초기화\n",
    "        sentences_pos = []\n",
    "        # 모든 문장 반복\n",
    "        for sentence in sentences:\n",
    "            # 특수기호 제거\n",
    "            sentence = re.sub(self.RE_FILTER, \"\", sentence)\n",
    "            # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "            sentence = \" \".join(tagger.morphs(sentence))\n",
    "            sentences_pos.append(sentence)\n",
    "        return sentences_pos\n",
    "    def startPosTag(self):\n",
    "        self.question = self.pos_tag(self.question)\n",
    "        self.answer = self.pos_tag(self.answer)\n",
    "    def setTrainSentences(self):\n",
    "        # 질문과 대답 문장들을 하나로 합침\n",
    "        self.sentences.extend(self.question)\n",
    "        self.sentences.extend(self.answer)\n",
    "        \n",
    "        # 단어들의 배열 생성\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence.split():\n",
    "                self.words.append(word)\n",
    "\n",
    "        # 길이가 0인 단어는 삭제\n",
    "        self.words = [word for word in self.words if len(word) > 0]\n",
    "\n",
    "        # 중복된 단어 삭제\n",
    "        self.words = list(set(self.words))\n",
    "\n",
    "        # 제일 앞에 태그 단어 삽입\n",
    "        self.words[:0] = [self.PAD, self.STA, self.END, self.OOV]\n",
    "    def makeDict(self):\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.words)}\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.words)}\n",
    "\n",
    "        with open('./json/' + datetime.today().strftime(\"%Y%m%d\") + '_' + 'word_to_index.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "            json.dump(self.word_to_index, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "\n",
    "        with open('./json/' + datetime.today().strftime(\"%Y%m%d\")+ '_' +'index_to_word.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "            json.dump(self.index_to_word, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "        # 문장을 인덱스로 변환\n",
    "    def convert_text_to_index(self, sentences, vocabulary, type): \n",
    "        sentences_index = []\n",
    "        # 모든 문장에 대해서 반복\n",
    "        for sentence in sentences:\n",
    "            sentence_index = []\n",
    "            # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
    "            if type == self.DECODER_INPUT:\n",
    "                sentence_index.extend([vocabulary[self.STA]])\n",
    "\n",
    "            # 문장의 단어들을 띄어쓰기로 분리\n",
    "            for word in sentence.split():\n",
    "                if vocabulary.get(word) is not None:\n",
    "                    # 사전에 있는 단어면 해당 인덱스를 추가\n",
    "                    sentence_index.extend([vocabulary[word]])\n",
    "                else:\n",
    "                    # 사전에 없는 단어면 OOV 인덱스를 추가\n",
    "                    sentence_index.extend([vocabulary[self.OOV]])\n",
    "\n",
    "            # 최대 길이 검사\n",
    "            if type == self.DECODER_TARGET:\n",
    "                # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
    "                if len(sentence_index) >= self.max_sequences:\n",
    "                    sentence_index = sentence_index[:self.max_sequences-1] + [vocabulary[self.END]]\n",
    "                else:\n",
    "                    sentence_index += [vocabulary[self.END]]\n",
    "            else:\n",
    "                if len(sentence_index) > self.max_sequences:\n",
    "                    sentence_index = sentence_index[:self.max_sequences]\n",
    "\n",
    "            # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
    "            sentence_index += (self.max_sequences - len(sentence_index)) * [vocabulary[self.PAD]]\n",
    "\n",
    "            # 문장의 인덱스 배열을 추가\n",
    "            sentences_index.append(sentence_index)\n",
    "\n",
    "        return np.asarray(sentences_index)\n",
    "    def makeEnDeCoder(self):\n",
    "        self.x_encoder = self.convert_text_to_index(self.question, self.word_to_index, self.ENCODER_INPUT)\n",
    "        self.x_decoder = self.convert_text_to_index(self.answer, self.word_to_index, self.DECODER_INPUT)\n",
    "        self.y_decoder = self.convert_text_to_index(self.answer, self.word_to_index, self.DECODER_TARGET)\n",
    "\n",
    "        x_en = pd.DataFrame(self.x_encoder)\n",
    "        x_en.to_csv('./EnDecoder/'+datetime.today().strftime(\"%Y%m%d\") + '_' +'x_encoder.csv')\n",
    "        x_de = pd.DataFrame(self.x_decoder)\n",
    "        x_de.to_csv('./EnDecoder/'+datetime.today().strftime(\"%Y%m%d\") + '_' +'x_decoder.csv')\n",
    "        y_de = pd.DataFrame(self.y_decoder)\n",
    "        y_de.to_csv('./EnDecoder/'+datetime.today().strftime(\"%Y%m%d\") + '_' +'y_decoder.csv')\n",
    "    def makeYdecoderToOneHot(self):\n",
    "        one_hot_data = np.zeros((len(self.y_decoder), self.max_sequences, len(self.words)))\n",
    "        # 디코더 목표를 원핫인코딩으로 변환\n",
    "        # 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n",
    "        for i, sequence in enumerate(self.y_decoder):\n",
    "            for j, index in enumerate(sequence):\n",
    "                one_hot_data[i, j, index] = 1\n",
    "\n",
    "        # 디코더 목표 설정\n",
    "        self.y_decoder = one_hot_data\n",
    "        \n",
    "    def trainModelInit(self):\n",
    "        self.encoder_inputs = layers.Input(shape=(None,))\n",
    "        encoder_outputs = layers.Embedding(len(self.words), self.embedding_dim)(self.encoder_inputs)\n",
    "\n",
    "        # return_state가 True면 상태값 리턴\n",
    "        # LSTM은 state_h(hidden state)와 state_c(cell state) 2개의 상태 존재\n",
    "        encoder_outputs, state_h, state_c = layers.LSTM(self.lstm_hidden_dim,\n",
    "                                                        dropout=0.1,\n",
    "                                                        recurrent_dropout=0.5,\n",
    "                                                        return_state=True)(encoder_outputs)\n",
    "\n",
    "        # 히든 상태와 셀 상태를 하나로 묶음\n",
    "        self.encoder_states = [state_h, state_c]\n",
    "\n",
    "        self.decoder_inputs = layers.Input(shape=(None,))\n",
    "\n",
    "        # 임베딩 레이어\n",
    "        self.decoder_embedding = layers.Embedding(len(self.words), self.embedding_dim)\n",
    "        decoder_outputs = self.decoder_embedding(self.decoder_inputs)\n",
    "\n",
    "        # 인코더와 달리 return_sequences를 True로 설정하여 모든 타임 스텝 출력값 리턴\n",
    "        # 모든 타임 스텝의 출력값들을 다음 레이어의 Dense()로 처리하기 위함\n",
    "        self.decoder_lstm = layers.LSTM(self.lstm_hidden_dim,\n",
    "                                   dropout=0.1,\n",
    "                                   recurrent_dropout=0.5,\n",
    "                                   return_state=True,\n",
    "                                   return_sequences=True)\n",
    "\n",
    "        # initial_state를 인코더의 상태로 초기화\n",
    "        decoder_outputs, _, _ = self.decoder_lstm(decoder_outputs,\n",
    "                                             initial_state=self.encoder_states)\n",
    "\n",
    "        # 단어의 개수만큼 노드의 개수를 설정하여 원핫 형식으로 각 단어 인덱스를 출력\n",
    "        self.decoder_dense = layers.Dense(len(self.words), activation='softmax')\n",
    "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
    "\n",
    "        # 입력과 출력으로 함수형 API 모델 생성\n",
    "        self.model = models.Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs)\n",
    "\n",
    "        # 학습 방법 설정\n",
    "        self.model.compile(optimizer='rmsprop',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])    \n",
    "        \n",
    "        self.model.save('./model/'+datetime.today().strftime(\"%Y%m%d\") + '_' + 'model.h5')\n",
    "    def predictModelInit(self):\n",
    "        # 훈련 모델의 인코더 상태를 사용하여 예측 모델 인코더 설정\n",
    "        self.encoder_model = models.Model(self.encoder_inputs, self.encoder_states)\n",
    "\n",
    "        # 예측시에는 훈련시와 달리 타임 스텝을 한 단계씩 수행\n",
    "        # 매번 이전 디코더 상태를 입력으로 받아서 새로 설정\n",
    "        decoder_state_input_h = layers.Input(shape=(self.lstm_hidden_dim,))\n",
    "        decoder_state_input_c = layers.Input(shape=(self.lstm_hidden_dim,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]    \n",
    "\n",
    "        # 임베딩 레이어\n",
    "        decoder_outputs = self.decoder_embedding(self.decoder_inputs)\n",
    "\n",
    "        # LSTM 레이어\n",
    "        decoder_outputs, state_h, state_c = self.decoder_lstm(decoder_outputs,\n",
    "                                                         initial_state=decoder_states_inputs)\n",
    "\n",
    "        # 히든 상태와 셀 상태를 하나로 묶음\n",
    "        decoder_states = [state_h, state_c]\n",
    "\n",
    "        # Dense 레이어를 통해 원핫 형식으로 각 단어 인덱스를 출력\n",
    "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
    "\n",
    "        # 예측 모델 디코더 설정\n",
    "        self.decoder_model = models.Model([self.decoder_inputs] + decoder_states_inputs,\n",
    "                              [decoder_outputs] + decoder_states)\n",
    "        self.encoder_model.save('./model/'+datetime.today().strftime(\"%Y%m%d\") + '_' + 'encoder_model.h5')\n",
    "        self.decoder_model.save('./model/'+datetime.today().strftime(\"%Y%m%d\") + '_' + 'decoder_model.h5')\n",
    "    def convert_index_to_text(self, indexs, vocabulary): \n",
    "    \n",
    "        sentence = ''\n",
    "\n",
    "        # 모든 문장에 대해서 반복\n",
    "        for index in indexs:\n",
    "            if index == self.END_INDEX:\n",
    "                # 종료 인덱스면 중지\n",
    "                break;\n",
    "            if vocabulary.get(index) is not None:\n",
    "                # 사전에 있는 인덱스면 해당 단어를 추가\n",
    "                sentence += vocabulary[index]\n",
    "            else:\n",
    "                # 사전에 없는 인덱스면 OOV 단어를 추가\n",
    "                sentence.extend([vocabulary[self.OOV_INDEX]])\n",
    "\n",
    "            # 빈칸 추가\n",
    "            sentence += ' '\n",
    "\n",
    "        return sentence\n",
    "    def startTrain(self, epoch, trainCount):\n",
    "        # 에폭 반복\n",
    "        sliceNum = int(trainCount / 10)\n",
    "        x_en = self.x_encoder[:sliceNum]\n",
    "        x_de = self.x_decoder[:sliceNum]\n",
    "        y_de = self.y_decoder[:sliceNum]\n",
    "        for epoch in range(epoch):\n",
    "            print('Total Epoch :', epoch + 1)\n",
    "            # 훈련 시작\n",
    "            history = self.model.fit([self.x_encoder[:trainCount], self.x_decoder[:trainCount]],\n",
    "                                self.y_decoder[:trainCount],\n",
    "                                epochs=1,\n",
    "                                batch_size=128,\n",
    "                                verbose=1,\n",
    "                                validation_data=([x_en,x_de], y_de)\n",
    "                               )\n",
    "\n",
    "            # 정확도와 손실 출력\n",
    "            print('accuracy :', history.history['accuracy'][-1])\n",
    "            print('loss :', history.history['loss'][-1])\n",
    "            print('val_acc : ', history.history['val_accuracy'][-1])\n",
    "            print('val_loss : ', history.history['val_loss'][-1])\n",
    "            self.acc.append(history.history['accuracy'])\n",
    "            self.loss.append(history.history['loss'])\n",
    "            self.v_acc.append(history.history['val_accuracy'])\n",
    "            self.v_loss.append(history.history['val_loss'])\n",
    "\n",
    "            # 문장 예측 테스트\n",
    "            # (3 박 4일 놀러 가고 싶다) -> (여행 은 언제나 좋죠)\n",
    "            input_encoder = self.x_encoder[2].reshape(1, self.x_encoder[2].shape[0])\n",
    "            input_decoder = self.x_decoder[2].reshape(1, self.x_decoder[2].shape[0])\n",
    "            results = self.model.predict([input_encoder, input_decoder])\n",
    "\n",
    "            # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "            # 1축을 기준으로 가장 높은 값의 위치를 구함\n",
    "            indexs = np.argmax(results[0], 1) \n",
    "\n",
    "            # 인덱스를 문장으로 변환\n",
    "            sentence = self.convert_index_to_text(indexs, self.index_to_word)\n",
    "            print(sentence)\n",
    "            print()\n",
    "            plt.figure(figsize=(25,10))\n",
    "            plt.subplot(121)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.plot(self.loss,  label = 'train_loss')\n",
    "            plt.plot(self.v_loss,  label = 'val_loss')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(122)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.plot(self.acc,  label = 'train_acc')\n",
    "            plt.plot(self.v_acc, label = 'val_acc')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    def make_predict_input(self, sentence):\n",
    "\n",
    "        sentences = []\n",
    "        sentences.append(sentence)\n",
    "        sentences = self.pos_tag(sentences)\n",
    "        input_seq = self.convert_text_to_index(sentences, self.word_to_index, self.ENCODER_INPUT)\n",
    "\n",
    "        return input_seq\n",
    "    # 텍스트 생성\n",
    "    def generate_text(self, input_seq):\n",
    "\n",
    "        # 입력을 인코더에 넣어 마지막 상태 구함\n",
    "        states = self.encoder_model.predict(input_seq)\n",
    "\n",
    "        # 목표 시퀀스 초기화\n",
    "        target_seq = np.zeros((1, 1))\n",
    "\n",
    "        # 목표 시퀀스의 첫 번째에 <START> 태그 추가\n",
    "        target_seq[0, 0] = self.STA_INDEX\n",
    "\n",
    "        # 인덱스 초기화\n",
    "        indexs = []\n",
    "\n",
    "        # 디코더 타임 스텝 반복\n",
    "        while 1:\n",
    "            # 디코더로 현재 타임 스텝 출력 구함\n",
    "            # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
    "            decoder_outputs, state_h, state_c = self.decoder_model.predict(\n",
    "                                                    [target_seq] + states)\n",
    "\n",
    "            # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "            index = np.argmax(decoder_outputs[0, 0, :])\n",
    "            indexs.append(index)\n",
    "\n",
    "            # 종료 검사\n",
    "            if index == self.END_INDEX or len(indexs) >= self.max_sequences:\n",
    "                break\n",
    "\n",
    "            # 목표 시퀀스를 바로 이전의 출력으로 설정\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = index\n",
    "\n",
    "            # 디코더의 이전 상태를 다음 디코더 예측에 사용\n",
    "            states = [state_h, state_c]\n",
    "\n",
    "        # 인덱스를 문장으로 변환\n",
    "        sentence = self.convert_index_to_text(indexs, self.index_to_word)\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "    def makePredictSentense(self,targetSentense):\n",
    "        return self.generate_text(self.make_predict_input(targetSentense))\n",
    "    \n",
    "    def makeOriginalSentenseDict(self, targetSentenseList):\n",
    "        for sentense in targetSentenseList :\n",
    "            self.originalDict[self.makePredictSentense(sentense)] = sentense\n",
    "            \n",
    "    def convert_to_originalSentense(self, predictSentense):\n",
    "        sentense = self.makePredictSentense(predictSentense)\n",
    "        return self.originalDict[sentense]\n",
    "    \n",
    "    def AutoStart(self, dataSet, epoch, trainCount):\n",
    "        for i in dataSet:\n",
    "            self.setDataSet(i)\n",
    "        self.startPosTag()\n",
    "        self.setTrainSentences()\n",
    "        self.makeDict()\n",
    "        self.makeEnDeCoder()\n",
    "        self.makeYdecoderToOneHot()\n",
    "        self.trainModelInit()\n",
    "        self.predictModelInit()\n",
    "        self.startTrain(epoch,trainCount)\n",
    "        self.makeOriginalSentenseDict(self.dataSet['A'][:trainCount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['./dataset/chatbot/예제.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_excel() got an unexpected keyword argument 'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4c41416fb9a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutoStart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainCount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-9f7f05f068bf>\u001b[0m in \u001b[0;36mAutoStart\u001b[1;34m(self, dataSet, epoch, trainCount)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mAutoStart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartPosTag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetTrainSentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-9f7f05f068bf>\u001b[0m in \u001b[0;36msetDataSet\u001b[1;34m(self, excelFile)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# 데이터 셋 집어넣을 때 반드시 컬럼명을 Q, A로 지정해주셔야 합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# 또한 엑셀파일 전용입니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexcelFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Q'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdataSetClean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 )\n\u001b[0;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: read_excel() got an unexpected keyword argument 'encoding'"
     ]
    }
   ],
   "source": [
    "cb.AutoStart(dataset, epoch= 10, trainCount=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습을 안시켰으니 이상한 결과가 나오는것은 당연합니다...\n",
    "\n",
    "cb.convert_to_originalSentense('안녕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
